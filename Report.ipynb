{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Report for Project 3: Collaboration and Competition\n",
    "\n",
    "\n",
    "## Learning Algorithm\n",
    "\n",
    "DDPG is used to train the agents. The DDPG architecture, including the local/target actor, local/target critic, and the replay buffer are shared among the two agents.\n",
    "\n",
    "At every time step, the network receives two copies of experience tuple, one from each agent, these two experience tuples are both added to a single replay buffer. Then if a training should be performed at this time step (determined by hyper parameters related to training frequency), a batch of samples is drawn uniformly from the replay buffer, (since the observation is stacked, the length of the state is 3 * 8), this batch is then used to train the DDPG agent.\n",
    "\n",
    "The DDPG trains as follows, given a batch of experiences, `(states, actions, rewards, next_states, dones)`, first the target q-value is calculated as:\n",
    "\n",
    "`y = rewards + (1 - dones) * gamma * critic_target(next_states, actor_target(next_states))`\n",
    "\n",
    "then the local critic is trained use the loss:\n",
    "\n",
    "`MSE(critic_local(states, actions), y)`\n",
    "\n",
    "the local actor is trained using the loss:\n",
    "\n",
    "`-reduce_mean(critic_local(states, actor_local(states)))`\n",
    "\n",
    "finally, if it satisfies the soft-update frequency, the local network is soft-copied to the target network.\n",
    "\n",
    "Architecture of the actor:\n",
    "\n",
    "```\n",
    "  # both fc1 and fc2 uses the ReLU activation,\n",
    "  # and fc3 uses tanh\n",
    "  \n",
    "  # states is passed to fc1 and follows down to the end\n",
    "  (fc1): Linear(in_features=24, out_features=400, bias=True)\n",
    "  (bn1): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (fc2): Linear(in_features=400, out_features=300, bias=True)\n",
    "  (fc3): NoisyLinear(in_features=300 out_features=2, bias=True)\n",
    "```\n",
    "\n",
    "Architecture of the critic:\n",
    "\n",
    "```\n",
    "  # all fc layers use the ReLU activation,\n",
    "  # except for the last one, for which there is no non-linearity\n",
    "  \n",
    "  # states is passed to fc1\n",
    "  (fc1): Linear(in_features=24, out_features=400, bias=True)\n",
    "  (bn1): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  \n",
    "  # the transformed states are merged with the actions,\n",
    "  # the merged tensors follows down to the end\n",
    "  (fc2): Linear(in_features=402, out_features=300, bias=True)\n",
    "  (fc3): Linear(in_features=300, out_features=1, bias=True)\n",
    "```\n",
    "\n",
    "| name                          | value    | comment|\n",
    "| ---                           | ---      | --- |\n",
    "| memory_max_size               | 1,000,000 | the size of the replay buffer |\n",
    "| num_episodes                  | 10000    | number of episodes to train |\n",
    "| batch_size                    | 128      | batch size |\n",
    "| gamma                         | 0.99     | reward decay |\n",
    "| critic_local_lr               | 1e-3     | critic's learning rate |\n",
    "| actor_local_lr                | 1e-3     | actor's learning rate |\n",
    "| update_target_every_learnings | 1        | the combination of `update_target_every_learnings=1`, `learn_every_new_experiences=20` and `times_consequtive_learn=10` means that for every 20 new samples, the local network is trained 10 times and the target network is updated 10 times|\n",
    "| learn_every_new_experiences   | 20       |  |\n",
    "| times_consequtive_learn       | 10       |  |\n",
    "| soft_update_tau               | 1e-3     | copy only this fraction of the local network to the target |\n",
    "\n",
    "\n",
    "## Plot of Rewards\n",
    "\n",
    "As can be seen from the plot, the environment is solved at around episode 8100 (got an average score of +0.5 over 100 consecutive episodes), and peaked episode 8270, which reached an average reward of +1.6\n",
    "\n",
    "![](run/Jul09_08-51-44_0.png)\n",
    "\n",
    "\n",
    "## Ideas for Future Work\n",
    "\n",
    "- Try prioritized experience replay\n",
    "- More stable learning"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}